# -*- coding: utf-8 -*-
"""Biblia embbeding gemma3 315m.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12WnsxJflnijSDDAJRO8lhMZ9rVv0R41D
"""

# CELDA 0 - (opcional) configuración rápida de colab
# Sube tu archivo .db (rvr60.sqlite3) con el panel de archivos de Colab
# Sube también aquí tu modelo de embeddings local a /content/model si lo tienes
# (o define HF_TOKEN en el entorno si quieres descargar desde HuggingFace)

# CELDA 1 - instalar dependencias
!pip install -q sentence-transformers huggingface-hub tqdm numpy sqlite3-binary

# Nota: sqlite3 viene con Python. "sqlite3-binary" es por si tu entorno no lo tuviera.
print("Dependencias instaladas")

# CELDA 2 - imports y configuración
import os, sqlite3, json, math, sys
from pathlib import Path
from tqdm import tqdm
import numpy as np
from sentence_transformers import SentenceTransformer
from huggingface_hub import login as hf_login
from typing import List, Tuple, Dict
import time
import base64

# --- Ajustes que puedes cambiar ---
DB_PATH = "rvr60.sqlite3"        # <- sube tu .db con este nombre o cámbialo
OUT_DB_PATH = "rvr60_embeddings.db"
MODEL_LOCAL_PATH = "/content/model"   # ruta si subes el modelo localmente
HF_MODEL_NAME = "embeddinggemma-300m" # nombre a usar si deseas descargar desde HuggingFace
USE_HF_DOWNLOAD = False              # Cambia a True si quieres que intente descargar (requiere HF_TOKEN)
BATCH_SIZE = 256                     # batch para encode
CHUNK_MAX = 3                        # max versos por chunk
TOP_K = 25                           # top resultados a devolver
EMBED_DTYPE = np.float32
# ------------------------------------

# util helpers (tomados y adaptados de tu código)
def float_verse_to_chap_verse(verse_str):
    if isinstance(verse_str, float) or isinstance(verse_str, (int,)):
        verse_str = f"{verse_str:.3f}"
    s = str(verse_str)
    if '.' in s:
        chap_s, verse_s = s.split('.', 1)
        try:
            ch = int(chap_s)
        except:
            ch = int(float(chap_s))
        try:
            v = int(verse_s)
        except:
            # si viene en formato decimal "1.001" ya es entero; si no, intentar derivar
            try:
                v = int(float("0." + verse_s) * 1000)
            except:
                v = int(float(verse_s))
        return ch, v
    else:
        return int(float(s)), 0

def serialize_vector(vec: np.ndarray) -> bytes:
    # guarda float32 en BLOB
    arr = np.asarray(vec, dtype=EMBED_DTYPE)
    return arr.tobytes()

def deserialize_vector(blob: bytes, dim: int) -> np.ndarray:
    return np.frombuffer(blob, dtype=EMBED_DTYPE).reshape((dim,))

def normalize(vecs: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    norms[norms==0] = 1.0
    return vecs / norms

print("Configuración lista.")

from huggingface_hub import login
from sentence_transformers import SentenceTransformer

# Reemplaza esto con tu token personal
login("hf_xxx")
model = SentenceTransformer("google/embeddinggemma-300m")

# # CELDA 3 - cargar modelo de embeddings (intenta local, luego HF si lo pediste)
# model = None
# try:
#     if os.path.exists(MODEL_LOCAL_PATH):
#         print(f"Cargando modelo local desde {MODEL_LOCAL_PATH} ...")
#         model = SentenceTransformer(MODEL_LOCAL_PATH)
#     elif USE_HF_DOWNLOAD:
#         print("Logueando a HF y descargando modelo...")
#         model = SentenceTransformer(HF_MODEL_NAME)
#     else:
#         # intento cargar por nombre (si Colab tiene internet y modelo público)
#         print(f"Intentando cargar modelo por nombre: {HF_MODEL_NAME} ...")
#         model = SentenceTransformer(HF_MODEL_NAME)
#     print("Modelo cargado. Dim embedding:", model.get_sentence_embedding_dimension())
# except Exception as e:
#     raise RuntimeError(f"No se pudo cargar el modelo de embeddings. Error: {e}")

# CELDA 4 - leer DB original y extraer versos ordenados por libro+verse
conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

# cargar tabla books para mapping osis->human y número
cur.execute("SELECT number, osis, human FROM books ORDER BY number")
books_rows = cur.fetchall()
osis_to_bookinfo = {}
for r in books_rows:
    osis_to_bookinfo[r["osis"]] = {"number": r["number"], "human": r["human"]}

# consulta general para obtener todos los versos ordenados por libro y verse
q = """
SELECT id, book, printf('%.3f', verse) as verse_str, unformatted
FROM verses
ORDER BY book, verse
"""
cur.execute(q)
rows = cur.fetchall()
if not rows:
    raise RuntimeError("No se encontraron versos en la tabla 'verses'. Verifica DB_PATH y estructura.")

# agrupar por libro y capítulo para conservar orden
books_data = {}  # book_osis -> {chap_num: [ {id, verse_num, text, verse_float_str} ... ]}
for r in rows:
    book_osis = r["book"]
    chap_num, verse_num = float_verse_to_chap_verse(r["verse_str"])
    books_data.setdefault(book_osis, {}).setdefault(chap_num, []).append({
        "id": r["id"],
        "verse_num": verse_num,
        "verse_str": r["verse_str"],
        "text": r["unformatted"] or ""
    })

print("Versos agrupados por libro y capítulo. Libros encontrados:", len(books_data))
conn.close()

# CELDA 5 - crear nueva DB para embeddings y estructura
if os.path.exists(OUT_DB_PATH):
    print("Atención: se sobrescribirá", OUT_DB_PATH)
    os.remove(OUT_DB_PATH)

out_conn = sqlite3.connect(OUT_DB_PATH)
out_cur = out_conn.cursor()

# tabla de vectores/embeddings
out_cur.execute("""
CREATE TABLE vectors (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    type TEXT NOT NULL,                -- 'verse' o 'chunk'
    verse_ids TEXT NOT NULL,           -- JSON array de ids originales (verso.id)
    book TEXT,                         -- book osis
    chapter INTEGER,
    verse_start INTEGER,
    verse_end INTEGER,
    text_concat TEXT,
    emb BLOB NOT NULL,
    dim INTEGER NOT NULL,
    created_at REAL
)
""")

out_cur.execute("CREATE INDEX idx_vectors_book ON vectors(book)")
out_cur.execute("CREATE INDEX idx_vectors_chapter ON vectors(chapter)")
out_conn.commit()
print("DB de embeddings creada:", OUT_DB_PATH)

# CELDA 6 - función para generar chunks (ventana deslizante contigua) y encolar segmentos a embebir
def produce_units_for_chapter(verse_entries: List[dict], max_chunk: int = 3):
    """
    verse_entries: lista ordenada de dicts {"id","verse_num","text","verse_str"}
    produce:
      - items individuales (type='verse', verse_ids=[id], text_concat=verse text)
      - chunks sliding windows de tamaño 2..max_chunk (type='chunk')
    """
    units = []
    n = len(verse_entries)
    # versos individuales
    for i, v in enumerate(verse_entries):
        units.append({
            "type": "verse",
            "verse_ids": [v["id"]],
            "book": None,
            "chapter": None,
            "verse_start": v["verse_num"],
            "verse_end": v["verse_num"],
            "text_concat": v["text"]
        })
    # chunks sliding windows para tamaños 2..max_chunk
    for k in range(2, max_chunk + 1):
        for i in range(0, n - k + 1):
            seg = verse_entries[i:i+k]
            vid = [s["id"] for s in seg]
            txt = "\n".join([s["text"] for s in seg])
            units.append({
                "type": "chunk",
                "verse_ids": vid,
                "book": None,
                "chapter": None,
                "verse_start": seg[0]["verse_num"],
                "verse_end": seg[-1]["verse_num"],
                "text_concat": txt
            })
    # mantener orden: versos individuales primero (ya agregado), luego chunks (ya agregado)
    return units

# CELDA 7 - encode por lotes y guardar en sqlite
out_conn.execute("BEGIN")
total_units = 0
start_time = time.time()
for book_osis in tqdm(sorted(books_data.keys()), desc="Libros"):
    for chap_num in sorted(books_data[book_osis].keys()):
        verse_entries = sorted(books_data[book_osis][chap_num], key=lambda x: x["verse_num"])
        units = produce_units_for_chapter(verse_entries, max_chunk=CHUNK_MAX)
        # rellenar book/chapter en unidades
        for u in units:
            u["book"] = book_osis
            u["chapter"] = chap_num

        # encode in batches
        texts = [u["text_concat"] for u in units]
        # por lotes
        for i in range(0, len(texts), BATCH_SIZE):
            batch_texts = texts[i:i+BATCH_SIZE]
            batch_units = units[i:i+BATCH_SIZE]
            emb_batch = model.encode(batch_texts, show_progress_bar=False, convert_to_numpy=True)
            # asegurar float32
            emb_batch = emb_batch.astype(EMBED_DTYPE)
            # guardar cada embedding en sqlite
            for bu, emb in zip(batch_units, emb_batch):
                b = serialize_vector(emb)
                verse_ids_json = json.dumps(bu["verse_ids"], separators=(',',':'))
                out_cur.execute("""
                    INSERT INTO vectors (type, verse_ids, book, chapter, verse_start, verse_end, text_concat, emb, dim, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    bu["type"],
                    verse_ids_json,
                    bu["book"],
                    bu["chapter"],
                    bu["verse_start"],
                    bu["verse_end"],
                    bu["text_concat"],
                    sqlite3.Binary(b),
                    emb.shape[0],
                    time.time()
                ))
                total_units += 1
        # commit per chapter para no mantener todo en memoria transaccional
        out_conn.commit()

elapsed = time.time() - start_time
print(f"Embeddings generados y guardados: {total_units}  (tiempo: {elapsed:.1f}s)")
out_conn.commit()

# CELDA 8 - carga en memoria para búsqueda (cargar IDs, verse_ids, tipo y vectores normalizados)
print("Cargando embeddings desde DB a memoria para búsqueda...")

out_cur.execute("SELECT id, type, verse_ids, book, chapter, verse_start, verse_end, text_concat, emb, dim FROM vectors")
rows = out_cur.fetchall()

ids = []
types = []
verse_ids_list = []
book_list = []
chapter_list = []
verse_start_list = []
verse_end_list = []
text_list = []
embs = []

for r in rows:
    ids.append(r[0])
    types.append(r[1])
    verse_ids_list.append(json.loads(r[2]))
    book_list.append(r[3])
    chapter_list.append(r[4])
    verse_start_list.append(r[5])
    verse_end_list.append(r[6])
    text_list.append(r[7] or "")
    blob = r[8]
    dim = r[9]
    vec = deserialize_vector(blob, dim)
    embs.append(vec)

embs = np.vstack(embs).astype(EMBED_DTYPE)
embs = normalize(embs)
print("Embeddings cargados:", embs.shape)

# CELDA 9 - funciones de búsqueda: embed de query, comparar por cosine, fusionar duplicados priorizando chunks
def embed_query(text: str) -> np.ndarray:
    v = model.encode([text], convert_to_numpy=True)[0].astype(EMBED_DTYPE)
    v = v / (np.linalg.norm(v) + 1e-12)
    return v

def top_k_search(query: str, k: int = TOP_K):
    qv = embed_query(query)
    # cosine similarity = dot (porque vectores ya normalizados)
    sims = embs.dot(qv)
    # obtener índices ordenados descendentes
    idxs = np.argsort(-sims)[:k*4]  # tomar más por si al fusionar se reducen
    results = []
    for idx in idxs:
        results.append({
            "index": int(idx),
            "db_id": ids[idx],
            "type": types[idx],
            "verse_ids": tuple(verse_ids_list[idx]),  # usar tupla para set/dict keys
            "book": book_list[idx],
            "chapter": chapter_list[idx],
            "verse_start": verse_start_list[idx],
            "verse_end": verse_end_list[idx],
            "text": text_list[idx],
            "score": float(sims[idx])
        })
    # Fusionar duplicados (mismo conjunto de verse_ids)
    merged = {}
    ordered = []
    for r in results:
        key = tuple(sorted(r["verse_ids"]))
        if key in merged:
            # prioriza chunk sobre verse: si ya existe y la nueva tiene mejor score y es chunk, sustituye.
            prev = merged[key]
            # si prev.type == 'verse' and r.type == 'chunk' -> sustituir siempre si score similar or higher
            if prev["type"] == "verse" and r["type"] == "chunk":
                merged[key] = r
        else:
            merged[key] = r
            ordered.append(key)
        # parar si ya tenemos k unicos
        if len(merged) >= k:
            break
    # confeccionar lista final ordenada por score descendente
    final = list(merged.values())
    final.sort(key=lambda x: x["score"], reverse=True)
    return final[:k]

# helper para mostrar referencias (buscamos nombre humano en books_rows)
osis_to_human = {r["osis"]: r["human"] for r in books_rows}

def human_reference(book_osis, chap, verse_start, verse_end):
    human_book = osis_to_human.get(book_osis, book_osis)
    if verse_start == verse_end:
        return f"{human_book} {chap}:{verse_start}"
    else:
        return f"{human_book} {chap}:{verse_start}-{verse_end}"

# CELDA 10 - interfaz simple para búsqueda (texto)
print("Listo. Usa la función `run_search(query)` para buscar.")

def run_search(query: str, topk:int = TOP_K, show_text=True):
    results = top_k_search(query, k=topk)
    out = []
    for idx, r in enumerate(results, start=1):
        ref = human_reference(r["book"], r["chapter"], r["verse_start"], r["verse_end"])
        display_text = r["text"] if show_text else ""
        out.append({
            "rank": idx,
            "score": r["score"],
            "type": r["type"],
            "reference": ref,
            "verse_ids": r["verse_ids"],
            "text": display_text
        })
    # imprimir bonito
    for e in out:
        print(f"{e['rank']:02d}. [{e['type']}] {e['reference']}  (score: {e['score']:.4f})")
        print(e['text'])
        print("-"*80)
    return out

# EJEMPLO de uso:
# resultado = run_search("Dios creó los cielos y la tierra", topk=25)
# (descomenta y ejecuta con tu consulta)